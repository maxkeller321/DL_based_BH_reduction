from math import pi, log
from functools import wraps

import torch
from torch import nn, einsum
import torch.nn.functional as F

from einops import rearrange, repeat
import pytorch_lightning as pl

# lamb
import collections
import math
from torch.optim import Optimizer

# metrics
from torchmetrics import MetricCollection
from torchmetrics.regression import MeanAbsoluteError
from torchmetrics import PSNR


# helpers
from visualization import make_grid, plot_pred_gt, plot_ct
from enum import Enum


class LossTypes(str, Enum):
    MSE = "mse"
    RMSE = "rmse"

    def __str__(self):
        return self.value


def exists(val):
    return val is not None


def default(val, d):
    return val if exists(val) else d


def cache_fn(f):
    cache = None

    @wraps(f)
    def cached_fn(*args, _cache=True, **kwargs):
        if not _cache:
            return f(*args, **kwargs)
        nonlocal cache
        if cache is not None:
            return cache
        cache = f(*args, **kwargs)
        return cache
    return cached_fn


# optimizer
# from: https://github.com/cybertronai/pytorch-lamb/blob/master/pytorch_lamb/lamb.py
# all rights with him
"""Lamb optimizer."""


def log_lamb_rs(optimizer: Optimizer, event_writer, token_count: int):
    """Log a histogram of trust ratio scalars in across layers."""
    results = collections.defaultdict(list)
    for group in optimizer.param_groups:
        for p in group['params']:
            state = optimizer.state[p]
            for i in ('weight_norm', 'adam_norm', 'trust_ratio'):
                if i in state:
                    results[i].append(state[i])


class Lamb(Optimizer):
    r"""Implements Lamb algorithm.
    It has been proposed in `Large Batch Optimization for Deep Learning: Training BERT in 76 minutes`_.
    Arguments:
        params (iterable): iterable of parameters to optimize or dicts defining
            parameter groups
        lr (float, optional): learning rate (default: 1e-3)
        betas (Tuple[float, float], optional): coefficients used for computing
            running averages of gradient and its square (default: (0.9, 0.999))
        eps (float, optional): term added to the denominator to improve
            numerical stability (default: 1e-8)
        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
        adam (bool, optional): always use trust ratio = 1, which turns this into
            Adam. Useful for comparison purposes.
    .. _Large Batch Optimization for Deep Learning: Training BERT in 76 minutes:
        https://arxiv.org/abs/1904.00962
    """

    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6,
                 weight_decay=0, adam=False):
        if not 0.0 <= lr:
            raise ValueError("Invalid learning rate: {}".format(lr))
        if not 0.0 <= eps:
            raise ValueError("Invalid epsilon value: {}".format(eps))
        if not 0.0 <= betas[0] < 1.0:
            raise ValueError(
                "Invalid beta parameter at index 0: {}".format(betas[0]))
        if not 0.0 <= betas[1] < 1.0:
            raise ValueError(
                "Invalid beta parameter at index 1: {}".format(betas[1]))
        defaults = dict(lr=lr, betas=betas, eps=eps,
                        weight_decay=weight_decay)
        self.adam = adam
        super(Lamb, self).__init__(params, defaults)

    def step(self, closure=None):
        """Performs a single optimization step.
        Arguments:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        """
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                grad = p.grad.data
                if grad.is_sparse:
                    raise RuntimeError(
                        'Lamb does not support sparse gradients, consider SparseAdam instad.')

                state = self.state[p]

                # State initialization
                if len(state) == 0:
                    state['step'] = 0
                    # Exponential moving average of gradient values
                    state['exp_avg'] = torch.zeros_like(p.data)
                    # Exponential moving average of squared gradient values
                    state['exp_avg_sq'] = torch.zeros_like(p.data)

                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
                beta1, beta2 = group['betas']

                state['step'] += 1

                # Decay the first and second moment running average coefficient
                # m_t
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                # v_t
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)

                # Paper v3 does not use debiasing.
                # bias_correction1 = 1 - beta1 ** state['step']
                # bias_correction2 = 1 - beta2 ** state['step']
                # Apply bias to lr to avoid broadcast.
                # * math.sqrt(bias_correction2) / bias_correction1
                step_size = group['lr']

                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)

                adam_step = exp_avg / exp_avg_sq.sqrt().add(group['eps'])
                if group['weight_decay'] != 0:
                    adam_step.add_(p.data, alpha=group['weight_decay'])

                adam_norm = adam_step.pow(2).sum().sqrt()
                if weight_norm == 0 or adam_norm == 0:
                    trust_ratio = 1
                else:
                    trust_ratio = weight_norm / adam_norm
                state['weight_norm'] = weight_norm
                state['adam_norm'] = adam_norm
                state['trust_ratio'] = trust_ratio
                if self.adam:
                    trust_ratio = 1

                p.data.add_(adam_step, alpha=-step_size * trust_ratio)

        return loss


# PerceiverIO by DeepMind

# Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, JoÃ£o Carreira.
# Perceiver: General Perception with Iterative Attention. ICML 2021. https://arxiv.org/abs/2103.03206

# Andrew Jaegle, Sebastian Borgeaud et al.
# Perceiver IO: A General Architecture for Structured Inputs & Outputs. arXiv, 2021. https://arxiv.org/abs/2107.14795

# from: https://github.com/lucidrains/perceiver-pytorch
# all rights with him

# helper classes

class PreNorm(pl.LightningModule):
    def __init__(self, dim, fn, context_dim=None):
        super().__init__()
        self.fn = fn
        self.norm = nn.LayerNorm(dim)
        self.norm_context = nn.LayerNorm(
            context_dim) if exists(context_dim) else None

    def forward(self, x, **kwargs):
        x = self.norm(x)

        if exists(self.norm_context):
            context = kwargs['context']
            normed_context = self.norm_context(context)
            kwargs.update(context=normed_context)

        return self.fn(x, **kwargs)


class GEGLU(pl.LightningModule):
    def forward(self, x):
        x, gates = x.chunk(2, dim=-1)
        return x * F.gelu(gates)


class FeedForward(pl.LightningModule):
    def __init__(self, dim, mult=4):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, dim * mult * 2),
            GEGLU(),
            nn.Linear(dim * mult, dim)
        )

    def forward(self, x):
        return self.net(x)


class Attention(pl.LightningModule):
    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64):
        super().__init__()
        inner_dim = dim_head * heads
        context_dim = default(context_dim, query_dim)
        self.scale = dim_head ** -0.5
        self.heads = heads

        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)
        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias=False)
        self.to_out = nn.Linear(inner_dim, query_dim)

    def forward(self, x, context=None, mask=None):
        h = self.heads

        q = self.to_q(x)
        context = default(context, x)
        k, v = self.to_kv(context).chunk(2, dim=-1)

        q, k, v = map(lambda t: rearrange(
            t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))

        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale

        # attention, what we cannot get enough of
        attn = sim.softmax(dim=-1)

        out = einsum('b i j, b j d -> b i d', attn, v)
        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)
        return self.to_out(out)

# main class


class PerceiverIO(pl.LightningModule):
    def __init__(
        self,
        *,
        depth,
        dim,
        queries_dim=torch.randn(1, 256, 32),
        logits_dim=None,
        num_latents=512,
        latent_dim=512,
        cross_heads=1,
        latent_heads=8,
        cross_dim_head=64,
        latent_dim_head=64,
        norm=False,
        plot_test_step=None,
        weight_tie_layers=False,
        decoder_ff=False,
        loss_function=LossTypes.MSE
    ):
        super().__init__()

        if (LossTypes.MSE == loss_function):
            self.loss_function = F.mse_loss
        elif (LossTypes.RMSE == loss_function):
            self.loss_function = lambda x, y: torch.sqrt(F.mse_loss(x, y))

        self.norm = norm
        self.plot_test_step = plot_test_step  # n-test images shall be plotted
        self.plot_val_cnt = 0
        self.plot_test_cnt = 0

        metrics = MetricCollection([PSNR(), MeanAbsoluteError()])
        self.train_metrics = metrics.clone(prefix='train_')
        self.val_metrics = metrics.clone(prefix='val_')
        self.test_metrics = metrics.clone(prefix='test_')
        self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))

        self.cross_attend_blocks = nn.ModuleList([
            PreNorm(latent_dim, Attention(
                latent_dim, dim, heads=cross_heads, dim_head=cross_dim_head), context_dim=dim),
            PreNorm(latent_dim, FeedForward(latent_dim))
        ])

        def get_latent_attn(): return PreNorm(latent_dim, Attention(
            latent_dim, heads=latent_heads, dim_head=latent_dim_head))

        def get_latent_ff(): return PreNorm(latent_dim, FeedForward(latent_dim))
        get_latent_attn, get_latent_ff = map(
            cache_fn, (get_latent_attn, get_latent_ff))

        self.layers = nn.ModuleList([])
        cache_args = {'_cache': weight_tie_layers}

        for i in range(depth):
            self.layers.append(nn.ModuleList([
                get_latent_attn(**cache_args),
                get_latent_ff(**cache_args)
            ]))

        self.decoder_cross_attn = PreNorm(queries_dim, Attention(
            queries_dim, latent_dim, heads=cross_heads, dim_head=cross_dim_head), context_dim=latent_dim)
        self.decoder_ff = PreNorm(queries_dim, FeedForward(
            queries_dim)) if decoder_ff else None

        self.to_logits = nn.Linear(queries_dim, logits_dim) if exists(
            logits_dim) else nn.Identity()

    def forward(
        self,
        batch,
        mask=None,
        queries=None
    ):
        data = batch.squeeze()
        b, *_, device = *data.shape, data.device

        x = repeat(self.latents, 'n d -> b n d', b=b)

        cross_attn, cross_ff = self.cross_attend_blocks

        # cross attention only happens once for Perceiver IO

        x = cross_attn(x, context=data, mask=mask) + x
        x = cross_ff(x) + x

        # layers

        for self_attn, self_ff in self.layers:
            x = self_attn(x) + x
            x = self_ff(x) + x

        if not exists(queries):
            return x

        # cross attend from decoder queries to latents

        latents = self.decoder_cross_attn(queries, context=x)

        # optional decoder feedforward

        if exists(self.decoder_ff):
            latents = latents + self.decoder_ff(latents)

        # final linear out

        return self.to_logits(latents)

    def configure_optimizers(self):
        # TODO: maybe use weight decay
        optimizer = Lamb(self.parameters(), lr=4e-4, weight_decay=0,
                         betas=(.9, .999), adam=True)
        return optimizer

    def training_step(self, data, batch_idx):
        # training_step defined the train loop.
        # It is independent of forward
        dataset_idx, batch = data
        x, y = batch
        x = x.squeeze()

        # Norm is not yet in use
        if self.norm:
            mean_x = torch.mean(x, [1, 2, 3])
            std_x = torch.std(x, [1, 2, 3], unbiased=False)
            std_x[std_x == 0] = 1e-5
            x = torch.div(
                torch.sub(x, mean_x[:, None, None, None]), std_x[:, None, None, None])
            mean_y = torch.mean(y, [1, 2, 3])
            std_y = torch.std(y, [1, 2, 3], unbiased=False)
            std_y[std_y == 0] = 1e-5
            y = torch.div(
                torch.sub(y, mean_y[:, None, None, None]), std_y[:, None, None, None])

        pred_bh = self(x)

        # get label image without neighbour slices
        #y_2 = torch.unsqueeze(y[:, 2, :, :], dim=1)
        true_bh = (y.squeeze()-x.squeeze())

        loss = self.loss_function(pred_bh, true_bh)

        if self.norm:
            pred_bh = torch.add(torch.multiply(
                pred_bh, std_x[:, None, None, None]), mean_x[:, None, None, None])
            true_bh = torch.add(torch.multiply(
                true_bh, std_y[:, None, None, None]), mean_y[:, None, None, None])

        self.log_dict(self.train_metrics(pred_bh, true_bh))
        self.log('train_loss', loss, sync_dist=True)
        self.logger.experiment.add_scalars(
            "losses", {"train_loss": loss}, global_step=self.global_step)
        return loss

    def validation_step(self, data, batch_idx):
        dataset_idx, batch = data
        x, y = batch

        pred_bh = self(x, dataset_idx)

        pred_bh = torch.unsqueeze(pred_bh, 1)

        # get label image without neighbour slices
        #y_2 = torch.unsqueeze(y[:, 2, :, :], dim=1)
        true_bh = (y.squeeze()-x.squeeze())

        loss = self.loss_function(pred_bh, true_bh)

        residual = x - pred_bh

        self.log_dict(self.val_metrics(residual, y))
        self.log('val_loss', loss, sync_dist=True)
        self.logger.experiment.add_scalars(
            "losses", {"val_loss": loss}, global_step=self.global_step)

        if self.plot_val_step is not None:
            # plot n-val images
            for idx in range(x.shape[0]):
                if self.plot_val_cnt > self.plot_val_step:
                    break
                self.show_pred_gt(x[idx, :, :, :],
                                  y[idx, :, :, :],
                                  y_hat_in=residual[idx, :, :, :],
                                  name=("val_img_"+str(self.plot_val_cnt)),
                                  use_global_step=True)
                self.plot_val_cnt += 1

    def on_validation_end(self) -> None:
        self.plot_val_cnt = 0

    def test_step(self, data, batch_idx):
        dataset_idx, batch = data
        x, y = batch

        pred_bh = self(x, dataset_idx)

        pred_bh = torch.unsqueeze(pred_bh, 1)

        # get label image without neighbour slices
        #y_2 = torch.unsqueeze(y[:, 2, :, :], dim=1)
        true_bh = (y.squeeze()-x.squeeze())

        loss = self.loss_function(pred_bh, true_bh)

        residual = x - pred_bh

        self.log_dict(self.test_metrics(residual, y))
        self.log('test_loss', loss, sync_dist=True)

        if self.plot_test_step is not None:
            # plot n-test images
            for idx in range(x.shape[0]):
                if self.plot_test_cnt > self.plot_test_step:
                    break
                self.show_pred_gt(x[idx, :, :, :],
                                  y[idx, :, :, :],
                                  y_hat_in=residual[idx, :, :, :],
                                  name=("test_img_"+str(self.plot_test_cnt)))
                self.plot_test_cnt += 1

    def show_pred_gt(self, x, y, y_hat_in=None, name="pred_gt", use_global_step=False):
        x = torch.unsqueeze(x, dim=0)
        y = torch.unsqueeze(y, dim=0)
        x_2 = torch.unsqueeze(x[:, :, :, :], dim=1)
        y_2 = torch.unsqueeze(y[:, :, :, :], dim=1)

        if y_hat_in is None:
            y_hat = self(x)
        else:
            y_hat = y_hat_in

        fig = plot_pred_gt(x_2, y_hat, y_2)
        if use_global_step:
            self.logger.experiment.add_figure(
                name, fig, global_step=self.global_step, close=True, walltime=None)
        else:
            self.logger.experiment.add_figure(
                name, fig, global_step=self.current_epoch, close=True, walltime=None)

# Perceiver LM example


class PerceiverLM(pl.LightningModule):
    def __init__(
        self,
        *,
        dim,
        num_tokens,
        max_seq_len,
        **kwargs
    ):
        super().__init__()
        self.token_emb = nn.Embedding(num_tokens, dim)
        self.pos_emb = nn.Embedding(max_seq_len, dim)

        self.perceiver_io = PerceiverIO(
            dim=dim,
            queries_dim=dim,
            logits_dim=num_tokens,
            **kwargs
        )

    def forward(
        self,
        x,
        mask=None
    ):
        n, device = x.shape[1], x.device
        x = self.token_emb(x)

        pos_emb = self.pos_emb(torch.arange(n, device=device))
        pos_emb = rearrange(pos_emb, 'n d -> () n d')
        x = x + pos_emb

        logits = self.perceiver_io(x, mask=mask, queries=x)
        return logits
